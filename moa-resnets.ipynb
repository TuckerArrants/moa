{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3",
      "language": "python"
    },
    "language_info": {
      "name": "python",
      "version": "3.7.6",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "colab": {
      "name": "moa-resnets.ipynb",
      "provenance": [],
      "private_outputs": true,
      "toc_visible": true,
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/TuckerArrants/moa/blob/main/moa-resnets.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-cL__dFZDCnb"
      },
      "source": [
        "!pip install --q iterative-stratification"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "trusted": true,
        "id": "BlzYUdvhCu2K"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd \n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn import preprocessing\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.feature_selection import VarianceThreshold\n",
        "from sklearn.decomposition import PCA\n",
        "from tensorflow.keras import layers,regularizers,Sequential,Model,backend,callbacks,optimizers,metrics,losses\n",
        "import tensorflow as tf\n",
        "from sklearn.metrics import log_loss\n",
        "from sklearn.preprocessing import RobustScaler\n",
        "from sklearn.decomposition import PCA\n",
        "import sys\n",
        "import json\n",
        "from iterstrat.ml_stratifiers import MultilabelStratifiedKFold\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "p_min = 0.0005\n",
        "p_max = 0.9995"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "_5DZcUFHCu2T"
      },
      "source": [
        "def preprocess(train,test):\n",
        "    \n",
        "    og_feats = [_ for _ in [_ for _ in train.columns if _ != 'sig_id'] if _ != 'cp_type']\n",
        "    train_non_ctl_idx = train.loc[train['cp_type'] != 'ctl_vehicle'].index.to_list()\n",
        "    train = train.loc[train_non_ctl_idx]\n",
        "    test_ctl_idx = test.loc[test['cp_type']=='ctl_vehicle'].index.to_list()\n",
        "    \n",
        "    train.drop(['sig_id','cp_type'],axis=1, inplace=True)\n",
        "    test.drop(['sig_id','cp_type'],axis=1, inplace=True)\n",
        "    \n",
        "    for df in [train, test]:\n",
        "        df.loc[:, 'cp_dose'] = df.loc[:, 'cp_dose'].map({'D1': 0, 'D2': 1})\n",
        "\n",
        "    scaler = preprocessing.StandardScaler()\n",
        "    train_ = scaler.fit_transform(train)\n",
        "    test_ = scaler.transform(test)\n",
        "    \n",
        "    train = pd.DataFrame(train_, index=train.index, columns=train.columns)\n",
        "    test = pd.DataFrame(test_, index=test.index, columns=test.columns)\n",
        "    \n",
        "    return train, test, og_feats, train_non_ctl_idx, test_ctl_idx\n",
        "\n",
        "def logloss(y_true, y_pred):\n",
        "    y_pred = tf.clip_by_value(y_pred,p_min,p_max)\n",
        "    return -backend.mean(y_true*backend.log(y_pred) + (1-y_true)*backend.log(1-y_pred))\n",
        "\n",
        "def variance_threshold_selector(data, threshold=0.8):\n",
        "    selector = VarianceThreshold(threshold)\n",
        "    selector.fit(data)\n",
        "    return data[data.columns[selector.get_support(indices=True)]]\n",
        "\n",
        "def fe_stats(train, test, extra=True):\n",
        "    \n",
        "    features_g = [col for col in train.columns if 'g-' in col]\n",
        "    features_c = [col for col in train.columns if 'c-' in col]\n",
        "\n",
        "    for df in [train, test]:\n",
        "        df['g_sum'] = df[features_g].sum(axis=1)\n",
        "        df['g_mean'] = df[features_g].mean(axis=1)\n",
        "        df['g_std'] = df[features_g].std(axis=1)\n",
        "        df['g_kurt'] = df[features_g].kurtosis(axis=1)\n",
        "        df['g_skew'] = df[features_g].skew(axis=1)\n",
        "        df['g_std'] = df[features_g].std(axis=1)\n",
        "        df['c_sum'] = df[features_c].sum(axis=1)\n",
        "        df['c_mean'] = df[features_c].mean(axis=1)\n",
        "        df['c_std'] = df[features_c].std(axis=1)\n",
        "        df['c_kurt'] = df[features_c].kurtosis(axis=1)\n",
        "        df['c_skew'] = df[features_c].skew(axis=1)\n",
        "        df['c_std'] = df[features_c].std(axis=1)\n",
        "        df['gc_sum'] = df[features_g + features_c].sum(axis=1)\n",
        "        df['gc_mean'] = df[features_g + features_c].mean(axis=1)\n",
        "        df['gc_std'] = df[features_g + features_c].std(axis=1)\n",
        "        df['gc_kurt'] = df[features_g + features_c].kurtosis(axis=1)\n",
        "        df['gc_skew'] = df[features_g + features_c].skew(axis=1)\n",
        "        df['gc_std'] = df[features_g + features_c].std(axis=1)\n",
        "        \n",
        "        #these might as well be arbitrary\n",
        "        if extra:\n",
        "            df['g_sum-c_sum'] = df['g_sum'] - df['c_sum']\n",
        "            df['g_mean-c_mean'] = df['g_mean'] - df['c_mean']\n",
        "            df['g_std-c_std'] = df['g_std'] - df['c_std']\n",
        "            df['g_kurt-c_kurt'] = df['g_kurt'] - df['c_kurt']\n",
        "            df['g_skew-c_skew'] = df['g_skew'] - df['c_skew']\n",
        "            \n",
        "            df['g_sum*c_sum'] = df['g_sum'] * df['c_sum']\n",
        "            df['g_mean*c_mean'] = df['g_mean'] * df['c_mean']\n",
        "            df['g_std*c_std'] = df['g_std'] * df['c_std']\n",
        "            df['g_kurt*c_kurt'] = df['g_kurt'] * df['c_kurt']\n",
        "            df['g_skew*c_skew'] = df['g_skew'] * df['c_skew']\n",
        "            \n",
        "            df['g_sum/c_sum'] = df['g_sum'] / df['c_sum']\n",
        "            df['g_mean/c_mean'] = df['g_mean'] / df['c_mean']\n",
        "            df['g_std/c_std'] = df['g_std'] / df['c_std']\n",
        "            df['g_kurt/c_kurt'] = df['g_kurt'] / df['c_kurt']\n",
        "            df['g_skew/c_skew'] = df['g_skew'] / df['c_skew']\n",
        "        \n",
        "    return train, test\n",
        "\n",
        "def fe_pca(train, test, n_components_g=70, n_components_c=10, SEED=123):\n",
        "    \n",
        "    features_g = [col for col in train.columns if 'g-' in col]\n",
        "    features_c = [col for col in train.columns if 'c-' in col]\n",
        "    \n",
        "    def create_pca(train, test, features, kind='g', n_components=n_components_g):\n",
        "        \n",
        "        train_ = train[features].copy()\n",
        "        test_ = test[features].copy()\n",
        "        data = pd.concat([train_, test_], axis=0)\n",
        "        pca = PCA(n_components=n_components,  random_state=SEED)\n",
        "        data = pca.fit_transform(data)\n",
        "        columns = [f'pca_{kind}{i + 1}' for i in range(n_components)]\n",
        "        data = pd.DataFrame(data, columns=columns)\n",
        "        train_ = data.iloc[:train.shape[0]]\n",
        "        test_ = data.iloc[train.shape[0]:].reset_index(drop=True)\n",
        "        train = pd.concat([train.reset_index(drop=True),\n",
        "                           train_.reset_index(drop=True)], axis=1)\n",
        "        test = pd.concat([test.reset_index(drop=True),\n",
        "                          test_.reset_index(drop=True)], axis=1)\n",
        "        return train, test\n",
        "    \n",
        "    train, test = create_pca(train, test, features_g, kind='g', n_components=n_components_g)\n",
        "    train, test = create_pca(train, test, features_c, kind='c', n_components=n_components_c)\n",
        "    \n",
        "    return train, test\n",
        "\n",
        "def mean_log_loss(y_true, y_pred):\n",
        "    y_pred = np.clip(y_pred, 1e-15, 1 - 1e-15)\n",
        "    metrics = []\n",
        "    for target in range(206):\n",
        "        metrics.append(log_loss(y_true[:, target], y_pred[:, target]))\n",
        "    return np.mean(metrics)\n",
        "\n",
        "def resnet_block(inputs1, inputs2, dropout, dim1=512, dim2=256):\n",
        "    block = tf.keras.layers.BatchNormalization()(inputs1)\n",
        "    block = tf.keras.layers.Dropout(0.2)(block)\n",
        "    block = tf.keras.layers.Dense(512, \"elu\")(block)\n",
        "    block = tf.keras.layers.BatchNormalization()(block)\n",
        "    block_out = tf.keras.layers.Dense(256, \"elu\")(block)\n",
        "    block_out_concat = tf.keras.layers.Concatenate()([inputs2, block])\n",
        "    return block_out, block_out_concat\n",
        "\n",
        "def build_resnet2(shape1, shape2, classes, LR=1e-3):  \n",
        "    \n",
        "    input_1 = tf.keras.layers.Input(shape=(shape1))\n",
        "    input_2 = tf.keras.layers.Input(shape=(shape2))\n",
        "    \n",
        "    block1_out, block1_out_concat = resnet_block(input_1, input_2, dropout=.2)\n",
        "    block2_out, _ = resnet_block(block1_out_concat, input_2, dropout=.3)\n",
        "    \n",
        "    block_avg = tf.keras.layers.Average()([block1_out, block2_out])\n",
        "    block_avg = tf.keras.layers.BatchNormalization()(block_avg)\n",
        "    block_avg = tf.keras.layers.Dense(256, kernel_initializer='lecun_normal', activation='selu')(block_avg)\n",
        "    block_avg = tf.keras.layers.BatchNormalization()(block_avg)\n",
        "    block_avg = tf.keras.layers.Dense(classes, kernel_initializer='lecun_normal', activation='selu')(block_avg)\n",
        "    block_avg = tf.keras.layers.BatchNormalization()(block_avg)\n",
        "    output = tf.keras.layers.Dense(classes,\"sigmoid\")(block_avg)\n",
        "\n",
        "    model = tf.keras.models.Model(inputs=[input_1, input_2], outputs=output)\n",
        "    adam = tf.optimizers.Adam(learning_rate=LR)\n",
        "    model.compile(optimizer = adam, \n",
        "                  loss=tf.keras.losses.BinaryCrossentropy(label_smoothing = 0.0015), \n",
        "                  metrics=tf.keras.metrics.BinaryCrossentropy())\n",
        "    \n",
        "    return model\n",
        "\n",
        "def build_resnet3(shape1, shape2, shape3, classes, LR=1e-3):  \n",
        "    \n",
        "    input_1 = tf.keras.layers.Input(shape=(shape1))\n",
        "    input_2 = tf.keras.layers.Input(shape=(shape2))\n",
        "    input_3 = tf.keras.layers.Input(shape=(shape3))\n",
        "    \n",
        "    block1_out, block1_out_concat = resnet_block(input_1, input_2, dropout=.2)\n",
        "    block2_out, block2_out_concat = resnet_block(block1_out_concat, input_3, dropout=.3)\n",
        "    block3_out, _ = resnet_block(block2_out_concat, input_3, dropout=.3)\n",
        "\n",
        "    block_avg = tf.keras.layers.Average()([block1_out, block2_out, block3_out])\n",
        "    block_avg = tf.keras.layers.BatchNormalization()(block_avg)\n",
        "    block_avg = tf.keras.layers.Dense(256, kernel_initializer='lecun_normal', activation='selu')(block_avg)\n",
        "    block_avg = tf.keras.layers.BatchNormalization()(block_avg)\n",
        "    block_avg = tf.keras.layers.Dense(classes, kernel_initializer='lecun_normal', activation='selu')(block_avg)\n",
        "    block_avg = tf.keras.layers.BatchNormalization()(block_avg)\n",
        "    output = tf.keras.layers.Dense(classes,\"sigmoid\")(block_avg)\n",
        "\n",
        "    model = tf.keras.models.Model(inputs=[input_1, input_2, input_3], outputs=output)\n",
        "    adam = tf.optimizers.Adam(learning_rate=LR)\n",
        "    model.compile(optimizer = adam, \n",
        "                  loss=tf.keras.losses.BinaryCrossentropy(label_smoothing = 0.0015), \n",
        "                  metrics=tf.keras.metrics.BinaryCrossentropy())\n",
        "    \n",
        "    return model\n",
        "\n",
        "start_predictors = [\"g-0\", \"g-7\", \"g-8\", \"g-10\", \"g-13\", \"g-17\", \"g-20\", \"g-22\", \"g-24\", \"g-26\", \"g-28\", \"g-29\", \"g-30\", \"g-31\", \"g-32\", \"g-34\", \"g-35\", \"g-36\", \"g-37\", \"g-38\",\n",
        "                    \"g-39\",\"g-41\", \"g-46\", \"g-48\", \"g-50\", \"g-51\", \"g-52\", \"g-55\", \"g-58\", \"g-59\", \"g-61\", \"g-62\", \"g-63\", \"g-65\", \"g-66\", \"g-67\", \"g-68\", \"g-70\", \"g-72\", \"g-74\", \n",
        "                    \"g-75\", \"g-79\", \"g-83\", \"g-84\", \"g-85\", \"g-86\", \"g-90\", \"g-91\", \"g-94\", \"g-95\", \"g-96\", \"g-97\", \"g-98\", \"g-100\", \"g-102\", \"g-105\", \"g-106\", \"g-112\", \"g-113\", \n",
        "                    \"g-114\", \"g-116\", \"g-121\", \"g-123\", \"g-126\", \"g-128\", \"g-131\", \"g-132\", \"g-134\", \"g-135\", \"g-138\", \"g-139\", \"g-140\", \"g-142\", \"g-144\", \"g-145\", \"g-146\", \n",
        "                    \"g-147\", \"g-148\", \"g-152\", \"g-155\", \"g-157\", \"g-158\", \"g-160\", \"g-163\", \"g-164\", \"g-165\", \"g-170\", \"g-173\", \"g-174\", \"g-175\", \"g-177\", \"g-178\", \"g-181\", \n",
        "                    \"g-183\", \"g-185\", \"g-186\", \"g-189\", \"g-192\", \"g-194\", \"g-195\", \"g-196\", \"g-197\", \"g-199\", \"g-201\", \"g-202\", \"g-206\", \"g-208\", \"g-210\", \"g-213\", \"g-214\", \n",
        "                    \"g-215\", \"g-220\", \"g-226\", \"g-228\", \"g-229\", \"g-235\", \"g-238\", \"g-241\", \"g-242\", \"g-243\", \"g-244\", \"g-245\", \"g-248\", \"g-250\", \"g-251\", \"g-254\", \"g-257\", \n",
        "                    \"g-259\", \"g-261\", \"g-266\", \"g-270\", \"g-271\", \"g-272\", \"g-275\", \"g-278\", \"g-282\", \"g-287\", \"g-288\", \"g-289\", \"g-291\", \"g-293\", \"g-294\", \"g-297\", \"g-298\",\n",
        "                    \"g-301\", \"g-303\", \"g-304\", \"g-306\", \"g-308\", \"g-309\", \"g-310\", \"g-311\", \"g-314\", \"g-315\", \"g-316\", \"g-317\", \"g-320\", \"g-321\", \"g-322\", \"g-327\", \"g-328\", \n",
        "                    \"g-329\", \"g-332\", \"g-334\", \"g-335\", \"g-336\", \"g-337\", \"g-339\", \"g-342\", \"g-344\", \"g-349\", \"g-350\", \"g-351\", \"g-353\", \"g-354\", \"g-355\", \"g-357\", \"g-359\", \n",
        "                    \"g-360\", \"g-364\", \"g-365\", \"g-366\", \"g-367\", \"g-368\", \"g-369\", \"g-374\", \"g-375\", \"g-377\", \"g-379\", \"g-385\", \"g-386\", \"g-390\", \"g-392\", \"g-393\", \"g-400\", \n",
        "                    \"g-402\", \"g-406\", \"g-407\", \"g-409\", \"g-410\", \"g-411\", \"g-414\", \"g-417\", \"g-418\", \"g-421\", \"g-423\", \"g-424\", \"g-427\", \"g-429\", \"g-431\", \"g-432\", \"g-433\", \n",
        "                    \"g-434\", \"g-437\", \"g-439\", \"g-440\", \"g-443\", \"g-449\", \"g-458\", \"g-459\", \"g-460\", \"g-461\", \"g-464\", \"g-467\", \"g-468\", \"g-470\", \"g-473\", \"g-477\", \"g-478\", \n",
        "                    \"g-479\", \"g-484\", \"g-485\", \"g-486\", \"g-488\", \"g-489\", \"g-491\", \"g-494\", \"g-496\", \"g-498\", \"g-500\", \"g-503\", \"g-504\", \"g-506\", \"g-508\", \"g-509\", \"g-512\", \n",
        "                    \"g-522\", \"g-529\", \"g-531\", \"g-534\", \"g-539\", \"g-541\", \"g-546\", \"g-551\", \"g-553\", \"g-554\", \"g-559\", \"g-561\", \"g-562\", \"g-565\", \"g-568\", \"g-569\", \"g-574\", \n",
        "                    \"g-577\", \"g-578\", \"g-586\", \"g-588\", \"g-590\", \"g-594\", \"g-595\", \"g-596\", \"g-597\", \"g-599\", \"g-600\", \"g-603\", \"g-607\", \"g-615\", \"g-618\", \"g-619\", \"g-620\", \n",
        "                    \"g-625\", \"g-628\", \"g-629\", \"g-632\", \"g-634\", \"g-635\", \"g-636\", \"g-638\", \"g-639\", \"g-641\", \"g-643\", \"g-644\", \"g-645\", \"g-646\", \"g-647\", \"g-648\", \"g-663\", \n",
        "                    \"g-664\", \"g-665\", \"g-668\", \"g-669\", \"g-670\", \"g-671\", \"g-672\", \"g-673\", \"g-674\", \"g-677\", \"g-678\", \"g-680\", \"g-683\", \"g-689\", \"g-691\", \"g-693\", \"g-695\", \n",
        "                    \"g-701\", \"g-702\", \"g-703\", \"g-704\", \"g-705\", \"g-706\", \"g-708\", \"g-711\", \"g-712\", \"g-720\", \"g-721\", \"g-723\", \"g-724\", \"g-726\", \"g-728\", \"g-731\", \"g-733\", \n",
        "                    \"g-738\", \"g-739\", \"g-742\", \"g-743\", \"g-744\", \"g-745\", \"g-749\", \"g-750\", \"g-752\", \"g-760\", \"g-761\", \"g-764\", \"g-766\", \"g-768\", \"g-770\", \"g-771\", \"c-0\", \n",
        "                    \"c-1\", \"c-2\", \"c-3\", \"c-4\", \"c-5\", \"c-6\", \"c-7\", \"c-8\", \"c-9\", \"c-10\", \"c-11\", \"c-12\", \"c-13\", \"c-14\", \"c-15\", \"c-16\", \"c-17\", \"c-18\", \"c-19\", \"c-20\", \n",
        "                    \"c-21\", \"c-22\", \"c-23\", \"c-24\", \"c-25\", \"c-26\", \"c-27\", \"c-28\", \"c-29\", \"c-30\", \"c-31\", \"c-32\", \"c-33\", \"c-34\", \"c-35\", \"c-36\", \"c-37\", \"c-38\", \"c-39\", \n",
        "                    \"c-40\", \"c-41\", \"c-42\", \"c-43\", \"c-44\", \"c-45\", \"c-46\", \"c-47\", \"c-48\", \"c-49\", \"c-50\", \"c-51\", \"c-52\", \"c-53\", \"c-54\", \"c-55\", \"c-56\", \"c-57\", \"c-58\", \n",
        "                    \"c-59\", \"c-60\", \"c-61\", \"c-62\", \"c-63\", \"c-64\", \"c-65\", \"c-66\", \"c-67\", \"c-68\", \"c-69\", \"c-70\", \"c-71\", \"c-72\", \"c-73\", \"c-74\", \"c-75\", \"c-76\", \"c-77\", \n",
        "                    \"c-78\", \"c-79\", \"c-80\", \"c-81\", \"c-82\", \"c-83\", \"c-84\", \"c-85\", \"c-86\", \"c-87\", \"c-88\", \"c-89\", \"c-90\", \"c-91\", \"c-92\", \"c-93\", \"c-94\", \"c-95\", \"c-96\", \n",
        "                    \"c-97\", \"c-98\", \"c-99\"]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CfOLdawJCu2Z"
      },
      "source": [
        "# Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "WN888yo1Cu2a"
      },
      "source": [
        "#these are dummy values\n",
        "model2 = build_resnet2(1000, 700, 206)\n",
        "model3 = build_resnet3(1000, 800, 700, 206)\n",
        "tf.keras.utils.plot_model(model2,show_shapes=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "flG0J4GBCu2h"
      },
      "source": [
        "tf.keras.utils.plot_model(model3,show_shapes=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ico715SsCu2n"
      },
      "source": [
        "# Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "2lzbRYTACu2n"
      },
      "source": [
        "TRAIN_BOTH = True\n",
        "TRANSFER = False\n",
        "VAR_THRESH = False\n",
        "ADD_PCA = False\n",
        "PCA_G_FEATS = 100\n",
        "PCA_C_FEATS = 10\n",
        "\n",
        "SEEDS = [34, 35, 36]\n",
        "FOLDS = 8\n",
        "EPOCHS = 10\n",
        "BATCH_SIZE = 128\n",
        "VERBOSE = 2\n",
        "\n",
        "#nonscored training params\n",
        "EPOCH_LIST = [5, 3, 3]\n",
        "BATCH_SIZE_LIST = [64, 128, 256]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "_kg_hide-output": true,
        "_kg_hide-input": true,
        "id": "zujW_xA4Cu2q"
      },
      "source": [
        "test = pd.read_csv('test_features.csv')\n",
        "oof_preds2 = []\n",
        "oof_preds3 = []\n",
        "seed_losses2 = []\n",
        "seed_losses3 = []\n",
        "test_pred = np.zeros((test.shape[0], 206)) \n",
        "\n",
        "for seed in SEEDS:\n",
        "\n",
        "    print(\"#\"*25)\n",
        "    print(f\"Training seed {seed}...\")\n",
        "    print(\"#\"*25)\n",
        "    print('')\n",
        "\n",
        "###################################################\n",
        "### Get data\n",
        "###################################################\n",
        "\n",
        "    train = pd.read_csv('train_features.csv')\n",
        "    train_targets = pd.read_csv('train_targets_scored.csv')\n",
        "    train_targets_ns = pd.read_csv('train_targets_nonscored.csv')\n",
        "    test = pd.read_csv('test_features.csv')\n",
        "    train_targets.drop('sig_id', axis=1, inplace=True)\n",
        "    train_targets_ns.drop('sig_id', axis=1, inplace=True)\n",
        "\n",
        "###################################################\n",
        "### Processes data\n",
        "###################################################\n",
        "\n",
        "    train, test = fe_stats(train, test)\n",
        "    train, test, og_feats, train_non_ctl_idx, test_ctl_idx = preprocess(train, test)\n",
        "    train_targets_ns = train_targets_ns.iloc[train_non_ctl_idx]\n",
        "    train_targets = train_targets.iloc[train_non_ctl_idx]\n",
        "\n",
        "    if ADD_PCA:\n",
        "        train, test = fe_pca(train, test, n_components_g=PCA_G_FEATS, n_components_c=PCA_C_FEATS, SEED=seed)\n",
        "\n",
        "    if VAR_THRESH:\n",
        "        total = pd.concat([train, test], ignore_index=True)\n",
        "        total = variance_threshold_selector(total, .8)\n",
        "        train = total[:len(train)]\n",
        "        test = total[len(train):]\n",
        "\n",
        "    all_feats = train.columns \n",
        "    oof_pred2 = np.zeros((train.shape[0], 206))\n",
        "    oof_pred3 = np.zeros((train.shape[0], 206))\n",
        "    \n",
        "    skf = MultilabelStratifiedKFold(n_splits=FOLDS, random_state=seed)\n",
        "\n",
        "    for f, (train_index, val_index) in enumerate(skf.split(train.values, train_targets.values)):\n",
        "\n",
        "        print(f\"Training fold {f}...\")\n",
        "        print('')\n",
        "\n",
        "        x_train, x_val = train[all_feats].values[train_index], train[all_feats].values[val_index]\n",
        "        y_train, y_val = train_targets.values[train_index], train_targets.values[val_index]\n",
        "        x_train_, x_val_ = train[og_feats].values[train_index], train[og_feats].values[val_index]\n",
        "        x_train__, x_val__ = train[start_predictors].values[train_index], train[start_predictors].values[val_index]\n",
        "\n",
        "###################################################\n",
        "### Build model\n",
        "###################################################\n",
        "\n",
        "        early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_binary_crossentropy',\n",
        "                                                          mode='min',\n",
        "                                                          patience=10,\n",
        "                                                          restore_best_weights=False,\n",
        "                                                          verbose=VERBOSE)\n",
        "\n",
        "        reduce_lr_loss = tf.keras.callbacks.ReduceLROnPlateau(monitor='val_binary_crossentropy',\n",
        "                                                              mode='min',\n",
        "                                                              factor=0.1,\n",
        "                                                              patience=4,\n",
        "                                                              verbose=VERBOSE)\n",
        "\n",
        "        sv2 = tf.keras.callbacks.ModelCheckpoint(f'resnet2_{f}_{seed}.h5',\n",
        "                                                monitor='val_binary_crossentropy',\n",
        "                                                verbose=VERBOSE,\n",
        "                                                save_best_only=True,\n",
        "                                                save_weights_only=True)\n",
        "\n",
        "        sv3 = tf.keras.callbacks.ModelCheckpoint(f'resnet3_{f}_{seed}.h5',\n",
        "                                                monitor='val_binary_crossentropy',\n",
        "                                                verbose=VERBOSE,\n",
        "                                                save_best_only=True,\n",
        "                                                save_weights_only=True)\n",
        "\n",
        "        sv2_ns = tf.keras.callbacks.ModelCheckpoint(f'resnet2_{f}_{seed}_ns.h5',\n",
        "                                                monitor='val_binary_crossentropy',\n",
        "                                                verbose=VERBOSE,\n",
        "                                                save_best_only=True,\n",
        "                                                save_weights_only=True)\n",
        "\n",
        "        sv3_ns = tf.keras.callbacks.ModelCheckpoint(f'resnet3_{f}_{seed}_ns.h5',\n",
        "                                                monitor='val_binary_crossentropy',\n",
        "                                                verbose=VERBOSE,\n",
        "                                                save_best_only=True,\n",
        "                                                save_weights_only=True)\n",
        "\n",
        "        resnet2 = build_resnet2(len(all_feats), len(start_predictors), train_targets.shape[1])\n",
        "        resnet3 = build_resnet3(len(all_feats), len(og_feats), len(start_predictors), train_targets.shape[1])\n",
        "\n",
        "###################################################\n",
        "### Train model and infer\n",
        "###################################################\n",
        "\n",
        "        if TRANSFER:\n",
        "\n",
        "            y_train_ns, y_val_ns = train_targets_ns.values[train_index], train_targets_ns.values[val_index]\n",
        "\n",
        "            resnet2_ns = build_resnet2(len(all_feats), len(start_predictors), train_targets_ns.shape[1])\n",
        "            resnet3_ns = build_resnet3(len(all_feats), len(og_feats), len(start_predictors), train_targets_ns.shape[1])\n",
        "            print('--'*25)\n",
        "            print('Training ResNet with 2 inputs on nonscored targets...')\n",
        "            print('--'*25)\n",
        "            for epoch, batch_size in zip(EPOCH_LIST, BATCH_SIZE_LIST):\n",
        "                resnet2_ns.fit([x_train, x_train__], y_train_ns,\n",
        "                            validation_data = ([x_val, x_val__], y_val_ns),\n",
        "                            epochs=epoch, \n",
        "                            batch_size=batch_size,\n",
        "                            callbacks=[sv2_ns, reduce_lr_loss],\n",
        "                            verbose=VERBOSE)\n",
        "                #resnet2_ns.fit([x_train, x_train], y_train_ns,\n",
        "                            #validation_data = ([x_val, x_val], y_val_ns),\n",
        "                            #epochs=epoch, \n",
        "                            #batch_size=batch_size,\n",
        "                            #callbacks=[sv2_ns, reduce_lr_loss],\n",
        "                            #verbose=VERBOSE)\n",
        "            resnet2_ns.load_weights(f'resnet2_{f}_{seed}_ns.h5')\n",
        "\n",
        "            if TRAIN_BOTH:\n",
        "                print('')\n",
        "                print('--'*25)\n",
        "                print('Training ResNet with 3 inputs on nonscored targets...')\n",
        "                print('--'*25)\n",
        "                print('')\n",
        "                for epoch, batch_size in zip(EPOCH_LIST, BATCH_SIZE_LIST):\n",
        "                    resnet3_ns.fit([x_train, x_train_, x_train__], y_train_ns,\n",
        "                                validation_data = ([x_val, x_val_, x_val__], y_val_ns),\n",
        "                                epochs=epoch, \n",
        "                                batch_size=batch_size,\n",
        "                                callbacks=[sv3_ns, reduce_lr_loss],\n",
        "                                verbose=VERBOSE)\n",
        "                    #resnet3_ns.fit([x_train, x_train, x_train], y_train_ns,\n",
        "                                #validation_data = ([x_val, x_val, x_val], y_val_ns),\n",
        "                                #epochs=epoch, \n",
        "                                #batch_size=batch_size,\n",
        "                                #callbacks=[sv3_ns, reduce_lr_loss],\n",
        "                                #verbose=VERBOSE)\n",
        "                print('')\n",
        "                resnet3_ns.load_weights(f'resnet3_{f}_{seed}_ns.h5')\n",
        "\n",
        "        print('--'*25)\n",
        "        print('Training ResNet with 2 inputs after transfering weights...' if TRANSFER else 'Training ResNet with 2 inputs...')\n",
        "        print('--'*25)\n",
        "        print('')\n",
        "\n",
        "        if TRANSFER:\n",
        "            for i, layer in enumerate(resnet2.layers[:-1]):\n",
        "                resnet2.layers[i] = resnet2_ns.layers[i]\n",
        "\n",
        "        resnet2.fit([x_train, x_train__], y_train,\n",
        "                    validation_data = ([x_val, x_val__], y_val),\n",
        "                    epochs=EPOCHS, \n",
        "                    batch_size=BATCH_SIZE,\n",
        "                    callbacks=[sv2, reduce_lr_loss],\n",
        "                    verbose=VERBOSE)\n",
        "        #resnet2.fit([x_train, x_train], y_train,\n",
        "                    #validation_data = ([x_val, x_val], y_val),\n",
        "                    #epochs=EPOCHS, \n",
        "                    #batch_size=BATCH_SIZE,\n",
        "                    #callbacks=[sv2, reduce_lr_loss],\n",
        "                    #verbose=VERBOSE)\n",
        "\n",
        "        print('')\n",
        "        resnet2.load_weights(f'resnet2_{f}_{seed}.h5')\n",
        "        oof_pred2[val_index] = resnet2.predict([x_val, x_val__])\n",
        "        test_pred += resnet2.predict([test[all_feats].values, test[start_predictors].values]) / (FOLDS * len(SEEDS))\n",
        "\n",
        "        if TRAIN_BOTH:\n",
        "            print('--'*25)\n",
        "            print('Training ResNet with 3 inputs after transfering weights...' if TRANSFER else 'Training ResNet with 3 inputs...') \n",
        "            print('--'*25)\n",
        "\n",
        "            if TRANSFER:\n",
        "                for i, layer in enumerate(resnet2.layers[:-1]):\n",
        "                    resnet2.layers[i] = resnet2_ns.layers[i]\n",
        "\n",
        "            resnet3.fit([x_train, x_train_, x_train__], y_train,\n",
        "                        validation_data = ([x_val, x_val_, x_val__], y_val),\n",
        "                        epochs=EPOCHS, \n",
        "                        batch_size=BATCH_SIZE,\n",
        "                        callbacks=[sv3, reduce_lr_loss],\n",
        "                        verbose=VERBOSE)\n",
        "            #resnet3.fit([x_train, x_train, x_train], y_train,\n",
        "                        #validation_data = ([x_val, x_val, x_val], y_val),\n",
        "                        #epochs=EPOCHS, \n",
        "                        #batch_size=BATCH_SIZE,\n",
        "                        #callbacks=[sv3, reduce_lr_loss],\n",
        "                        #verbose=VERBOSE)\n",
        "            print('')   \n",
        "\n",
        "            resnet3.load_weights(f'resnet3_{f}_{seed}.h5')\n",
        "            oof_pred3[val_index] = resnet3.predict([x_val, x_val_, x_val__])\n",
        "            test_pred += resnet3.predict([test[all_feats].values, test[og_feats].values, test[start_predictors].values]) / (FOLDS * len(SEEDS))\n",
        "\n",
        "    oof_preds2.append(oof_pred2)\n",
        "    seed_loss2 = mean_log_loss(train_targets.values, oof_pred2)\n",
        "    seed_losses2.append(seed_loss2)\n",
        "    print(f\"ResNet with 2 inputs {seed} mean log loss: {seed_loss2}\")\n",
        "    print('')\n",
        "\n",
        "    if TRAIN_BOTH:\n",
        "        oof_preds3.append(oof_pred3)\n",
        "        seed_loss3 = mean_log_loss(train_targets.values, oof_pred3)\n",
        "        seed_losses3.append(seed_loss3)\n",
        "        print(f\"ResNet with 3 inputs {seed} mean log loss: {seed_loss3}\")\n",
        "        print('')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "Yjp-fiqXCu2t"
      },
      "source": [
        "print('ResNet with 2 inputs results:')\n",
        "for i, score in enumerate(seed_losses2):\n",
        "    print(f\"Seed {SEEDS[i]} score: {score}\")\n",
        "\n",
        "oof_preds2_ = np.zeros((train.shape[0], 206)) \n",
        "for _ in oof_preds2:\n",
        "    oof_preds2_ += _\n",
        "oof_preds2_ *= 1/len(SEEDS)\n",
        "\n",
        "overall_loss2 = mean_log_loss(train_targets.values, oof_preds2_)\n",
        "\n",
        "print(f\"Overall loss: {overall_loss2}\")\n",
        "print('')\n",
        "\n",
        "if TRAIN_BOTH:\n",
        "    print('ResNet with 3 inputs results:')\n",
        "    for i, score in enumerate(seed_losses3):\n",
        "        print(f\"Seed {SEEDS[i]} score: {score}\")\n",
        "\n",
        "    oof_preds3_ = np.zeros((train.shape[0], 206)) \n",
        "    for _ in oof_preds3:\n",
        "        oof_preds3_ += _\n",
        "    oof_preds3_ *= 1/len(SEEDS)\n",
        "\n",
        "    overall_loss3 = mean_log_loss(train_targets.values, oof_preds3_)\n",
        "\n",
        "    print(f\"Overall loss: {overall_loss3}\")\n",
        "\n",
        "    overall_loss = mean_log_loss(train_targets.values, (oof_preds2_ + oof_preds3_) / 2)\n",
        "    print(f'Blended loss : {overall_loss}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "_aIQBA8PCu2v"
      },
      "source": [
        "if TRAIN_BOTH:\n",
        "  test_pred *= 1/2\n",
        "sub = pd.read_csv('sample_submission.csv')\n",
        "sub.iloc[:,1:] = np.clip(test_pred,p_min,p_max)\n",
        "sub.iloc[test_ctl_idx,1:] = 0\n",
        "sub.to_csv('submission.csv', index=False)\n",
        "sub"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}